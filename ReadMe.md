## this repositiory is experiments on self distillation model
this repository hierarchical loss in self distillation model.
## reference:
1. [Self-Distillation with Meta Learning for Knowledge Graph Completion](https://aclanthology.org/2022.findings-emnlp.149/)
2. [Revisiting Self Distillation](https://arxiv.org/abs/2206.08491)
3. [Be Your Own Teacher: Improve the Performance of Convolutional Neural
Networks via Self Distillation](https://arxiv.org/abs/1905.08094)
4. [Distilling a Neural Network Into a Soft Decision Tree](https://arxiv.org/abs/1711.09784)
5. [FastBERT a Self-distilling BERT with Adaptive Inference Time](https://arxiv.org/abs/2004.02178)
6. [Self-Distillation: Towards Efficient and Compact Neural Networks](https://ieeexplore.ieee.org/document/9381661)
7. [Spatial self-attention network with self-attention distillation for fine-grained image recognition](https://www.sciencedirect.com/science/article/pii/S104732032100242X)
8. [Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning](https://arxiv.org/abs/2012.09816)

## other reference
[Kullbackâ€“Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)

## related repositories
1. [K-Bert](https://github.com/autoliuweijie/K-BERT)
    inject knowledge graph into Bert
2. [FastBert](https://github.com/autoliuweijie/FastBERT)
    Self Distillation Bert